{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae34f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# You need to import custom class for loading pickle file to work.\n",
    "from ml_runner.v1 import DataHandler, MyEval, MyUtil, RegSwitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "IS_PLOT_RES = False\n",
    "SAVE_PLOT_RES = False\n",
    "\n",
    "# Combined plots\n",
    "SAVE_PLOT = True\n",
    "\n",
    "# Data\n",
    "SAVE_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f9b2c",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80adc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for pkl files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "onlyfiles = [f for f in listdir(\".\") if (isfile(join(\".\", f)) and f.endswith(\"pkl\"))]\n",
    "pp(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenameInput = \"S04_data_2025-05-18_10-34.pkl\"\n",
    "data_load = MyUtil.load_data(filename=filenameInput)\n",
    "\n",
    "# Print keys\n",
    "pp([k for k in data_load.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = MyUtil.get_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = data_load[\"data_handler\"]\n",
    "df_cv = data_load[\"df_cv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959fb424",
   "metadata": {},
   "source": [
    "### Calculate test results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by \"rank_test_score\"\n",
    "df_cv = df_cv.sort_values(by=\"rank_test_score\")\n",
    "\n",
    "# Groups the sorted DataFrame by the columns \"id_split\" and \"estimator\".\n",
    "# For each group (unique combination of split and estimator), selects the first row (which, after sorting, is the one with the best rank_test_score).\n",
    "# .reset_index() turns the groupby indices back into columns for a clean DataFrame.\n",
    "df_fit_select = df_cv.groupby([\"id_split\", \"estimator\"]).first().reset_index()\n",
    "\n",
    "display(df_fit_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize blank model (optional)\n",
    "reg = RegSwitcher(base=None)\n",
    "\n",
    "\n",
    "df_arr = []\n",
    "for idx, fit in df_fit_select.iterrows():\n",
    "    # pp(fit[\"param_split\"])\n",
    "    # pp(fit[\"params\"])\n",
    "\n",
    "    param_split = fit[\"param_split\"]\n",
    "    data_handler.split_and_scale(**param_split)\n",
    "\n",
    "    X_train, Y_train = data_handler.get_train()\n",
    "    X_test, Y_test = data_handler.get_test()\n",
    "\n",
    "    params = fit[\"params\"]\n",
    "    reg.set_params(**params)\n",
    "\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    Y_train_pred = reg.predict(X_train)\n",
    "    Y_test_pred = reg.predict(X_test)\n",
    "\n",
    "    _df = MyEval.eval(\n",
    "        Y_train=Y_train,\n",
    "        Y_train_pred=Y_train_pred,\n",
    "        Y_test=Y_test,\n",
    "        Y_test_pred=Y_test_pred,\n",
    "        id_split=fit[\"id_split\"],\n",
    "        id_gs=fit[\"id_gs\"],\n",
    "        estimator=fit[\"estimator\"],\n",
    "    )\n",
    "    df_arr.append(_df)\n",
    "\n",
    "    if IS_PLOT_RES:\n",
    "        id_split = fit[\"id_split\"]\n",
    "        estimator = fit[\"estimator\"]\n",
    "        MyEval.plot_res(\n",
    "            Y_train=Y_train,\n",
    "            Y_train_pred=Y_train_pred,\n",
    "            Y_test=Y_test,\n",
    "            Y_test_pred=Y_test_pred,\n",
    "            dt=dt,\n",
    "            save=SAVE_PLOT_RES,\n",
    "            file_prefix=f\"S05-{estimator}-{id_split}\",\n",
    "        )\n",
    "\n",
    "df_eval = pd.concat(df_arr).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7e1e2",
   "metadata": {},
   "source": [
    "### Further data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "# - **After this code runs:**\n",
    "#   - `df_eval` will have a new column, `validation_scores`, containing values from `df_cv` where the keys match.\n",
    "#   - All original rows in `df_eval` are preserved.\n",
    "  \n",
    "# - **What it does:**  \n",
    "#   - Sets the value of the `\"Y\"` column in the `df_cv` DataFrame to `\"Y-All\"` for all rows.\n",
    "#   - If the `\"Y\"` column does not exist, it is created.\n",
    "df_cv[\"Y\"] = \"Y-All\"\n",
    "\n",
    "# - **What it does:**  \n",
    "#   - Defines a list of column names that will be used from `df_cv` when merging with `df_eval`.\n",
    "#   - These columns are: `id_split`, `id_gs`, `Y`, and `validation_scores`.\n",
    "colsToMerge = [\"id_split\", \"id_gs\", \"Y\", \"validation_scores\"]\n",
    "\n",
    "# - **What it does:**  \n",
    "#   - Merges the `df_eval` DataFrame with a subset of `df_cv` (only the columns in `colsToMerge`).\n",
    "#   - The merge is performed on the columns `id_split`, `id_gs`, and `Y`.\n",
    "#   - `how=\"left\"` specifies a left join, meaning all rows from `df_eval` are kept, and matching rows from `df_cv` are added where available.\n",
    "#   - If there is no match in `df_cv`, the new columns (e.g., `validation_scores`) will have `NaN` values in `df_eval`.\n",
    "df_eval = df_eval.merge(df_cv[colsToMerge], on=[\"id_split\", \"id_gs\", \"Y\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# - **What this function does:**\n",
    "#   - Takes a DataFrame `_df` (expected to be a group from a groupby operation).\n",
    "#   - Extracts the `\"validation_scores\"` column, which contains arrays or lists.\n",
    "#   - Concatenates all arrays/lists into a single NumPy array (`val_scores`).\n",
    "#   - Returns a new DataFrame with one column, `\"cv_results\"`, containing all the concatenated scores.\n",
    "def expandCV(_df):\n",
    "    val_scores = np.concatenate(_df[\"validation_scores\"].values)\n",
    "    return pd.DataFrame(data={\"cv_results\": val_scores})\n",
    "\n",
    "\n",
    "#   - Creates a boolean mask that is `True` where the `\"Y\"` column equals `\"Y-All\"`.\n",
    "#   - This mask is used to filter the DataFrame for relevant rows.\n",
    "filt = df_eval[\"Y\"] == \"Y-All\"\n",
    "\n",
    "# - **Step-by-step:**\n",
    "#   1. **Filter Rows:**\n",
    "#      `df_eval[filt]` selects only rows where `\"Y\" == \"Y-All\"`.\n",
    "#   2. **Group by Estimator:**\n",
    "#      `.groupby(by=[\"estimator\"])` groups the filtered DataFrame by the `\"estimator\"` column.\n",
    "#   3. **Apply `expandCV`:**\n",
    "#      `.apply(expandCV, include_groups=False)` applies the `expandCV` function to each group, expanding the cross-validation results into individual rows.\n",
    "#   4. **Reset Index:**\n",
    "#      `.reset_index(drop=False)` resets the index so that group labels become columns.\n",
    "#   5. **Drop Extra Column:**\n",
    "#      `.drop(columns=[\"level_1\"])` removes the `\"level_1\"` column, which is a byproduct of the groupby-apply process.\n",
    "cv_data = (\n",
    "    df_eval[filt]\n",
    "    .groupby(by=[\"estimator\"])\n",
    "    .apply(expandCV, include_groups=False)\n",
    "    .reset_index(drop=False)\n",
    "    .drop(columns=[\"level_1\"])\n",
    ")\n",
    "\n",
    "display(cv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13189e5f",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a11f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "# Plot CV results\n",
    "sns.boxplot(cv_data, x=\"estimator\", y=\"cv_results\", ax=axes[0])\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].set_title(\"CV Results\")\n",
    "\n",
    "# Plot test results\n",
    "ax = sns.boxplot(data=df_eval, x=\"estimator\", y=\"R2 Test\", hue=\"Y\", ax=axes[1])\n",
    "axes[1].set_title(\"Test Result\")\n",
    "\n",
    "# Plot train (no cv) results\n",
    "sns.boxplot(data=df_eval, x=\"estimator\", y=\"R2 Train (No Val)\", hue=\"Y\", ax=axes[2])\n",
    "axes[2].set_title(\"Train Result (No Validation)\")\n",
    "\n",
    "if SAVE_PLOT:\n",
    "    filename = f\"S05_eval_{dt}.png\"\n",
    "    fig.savefig(filename, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d579a",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DATA:\n",
    "    filename = f\"S04_data_{dt}.pkl\"\n",
    "\n",
    "    data_save = {\n",
    "        \"desc\": \"This is the saved data\",\n",
    "        \"filenameInput\": filename,\n",
    "        \"df_eval\": df_eval,\n",
    "        \"cv_data\": cv_data\n",
    "    }\n",
    "\n",
    "    # Save the model\n",
    "    MyUtil.save_data(filename=filename, data=data_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc7078",
   "metadata": {},
   "source": [
    "### Test loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e99991",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DATA:\n",
    "    data_load = MyUtil.load_data(filename=filename)\n",
    "\n",
    "    pp(list(data_load.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
